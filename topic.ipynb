{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://earlyprint.org/jupyterbook/unsupervised.html\n",
    "'''\n",
    "# General Libraries Needed\n",
    "import csv\n",
    "import os \n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Functions for Unsupervised Clustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Libraries for Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def getTexts(folder):\n",
    "    '''\n",
    "    Takes in plain text files and outputs a tuple of lists, with the first being the text\n",
    "    within each file as a string and the second list being the IDs of each text. \n",
    "    '''\n",
    "    textStrings = []\n",
    "    fileNames = []\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder,file)\n",
    "        f = open(path,'r')\n",
    "        text = f.readlines()[0]\n",
    "        textStrings.append(text)\n",
    "        name = file.split('.')[0]\n",
    "        fileNames.append(name)\n",
    "        f.close()\n",
    "    return textStrings,fileNames\n",
    "\n",
    "def vectorize(strings,ids):\n",
    "    # create vectorizer instance w/ normalization set to l2 by default \n",
    "    tfidf = TfidfVectorizer(min_df=2, sublinear_tf=True)\n",
    "    # I am choosing to turn on sublinear term frequency scaling, which takes the log of\n",
    "    # term frequencies and can help to de-emphasize function words like pronouns and articles. \n",
    "    # You might make a different choice depending on your corpus.\n",
    "\n",
    "    # Once we've created the instance, we can \"transform\" our counts\n",
    "    results = tfidf.fit_transform(strings)\n",
    "    # Make results readable using Pandas\n",
    "    return pd.DataFrame(results.toarray(), index=ids, columns=tfidf.get_feature_names_out()) # Convert information back to a DataFrame\n",
    "\n",
    "def cluster(df,ids,num):\n",
    "    # Create a KMeans instance that will look for a specified number of clusters\n",
    "    # TODO: understand what the random_state parameter does  \n",
    "    kmeans = KMeans(n_clusters=num, random_state=42) \n",
    "    kmeans.fit(df) # Feed in our normalized data\n",
    "\n",
    "    kmeans_groups = defaultdict(list)\n",
    "    for k,v in zip(kmeans.labels_,ids):\n",
    "        kmeans_groups[k].append(v)\n",
    "        \n",
    "    for v in kmeans_groups.values():\n",
    "        print(v)\n",
    "\n",
    "    # Calculate PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_results = pca.fit_transform(df)\n",
    "\n",
    "    # Put PCA into a DataFrame\n",
    "    pca_df = pd.DataFrame(pca_results, index=ids, columns=[\"pc1\",\"pc2\"])\n",
    "\n",
    "    # Add \"color\" column for K-Means groups\n",
    "    pca_df['color'] = pd.Series(kmeans.labels_, index=ids)\n",
    "    pca_df.plot.scatter(x='pc1', y='pc2', c='color', colormap='tab10', colorbar=False)\n",
    "    return kmeans_groups\n",
    "\n",
    "def keywords(csv,groups):\n",
    "    df = pd.read_csv(csv)\n",
    "    keywords = df['keywords']\n",
    "    ids = df['id']\n",
    "    numFiles = len(ids)\n",
    "    count = 0\n",
    "    dict = {}\n",
    "    while count < numFiles:\n",
    "        words = set(keywords[count].split('--'))\n",
    "        words.discard('')\n",
    "        dict[ids[count]] = words\n",
    "        count += 1\n",
    "    for k,v in groups.items():\n",
    "        print(f'Group {k+1}')\n",
    "        keyterms = []\n",
    "        for name in v: \n",
    "            keyterms.extend(dict[name])\n",
    "        print(Counter(keyterms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/srv/data/texts'\n",
    "csv = '/srv/data/CSVs/EPtuning.csv'\n",
    "info = getTexts(folder)\n",
    "df = vectorize(info[0],info[1])\n",
    "groups = cluster(df,info[1],4)\n",
    "keywords(csv,groups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "163b8b902ef20465a7ecf57c45ba2fc54a366f8c8d80240b381c6f4fa0585aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

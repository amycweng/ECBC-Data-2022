{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Unsupervised k-means clustering on standardized and cleaned plain text file versions of EEBO-TCP texts. \n",
    "\n",
    "Produces a user-specified number of clusters based on term frequency vectorization. \n",
    "These clusters are described using the keywords found in their corresponding entries \n",
    "in a metadata CSV file made using metadata.py in Stage I. \n",
    "\n",
    "The vectorize and cluster functions are adapted from the EarlyPrint Lab: \n",
    "    https://earlyprint.org/jupyterbook/unsupervised.html\n",
    "The topTerms function is adapted from \n",
    "    https://pythonprogramminglanguage.com/kmeans-text-clustering/ \n",
    "The elbow and intercluster functions are adapted from Yellowbrick's documentation: \n",
    "    https://www.scikit-yb.org/en/latest/index.html\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict,Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "def vectorize(strings,ids,option):\n",
    "    '''\n",
    "    Vectorizes the texts and returns a dataframe of texts mapping to their TCP IDs. \n",
    "    '''\n",
    "    # vector = TfidfVectorizer(min_df=2, sublinear_tf=True)\n",
    "    # results = vector.fit_transform(strings)\n",
    "    if option =='count': vector = CountVectorizer()\n",
    "    else: vector = TfidfVectorizer()\n",
    "    results = vector.fit_transform(strings)\n",
    "    return pd.DataFrame(results.toarray(), index=ids, columns=vector.get_feature_names_out()) # Convert information back to a DataFrame\n",
    "\n",
    "def cluster(df,ids,num):\n",
    "    model = KMeans(n_clusters=num, random_state=42) \n",
    "    model.fit(df) \n",
    "    kmeans_groups = defaultdict(list)\n",
    "    for k,v in zip(model.labels_,ids):\n",
    "        kmeans_groups[k].append(v)\n",
    "    return kmeans_groups,model\n",
    "\n",
    "def topTerms(model,df,num):\n",
    "    tops = {}\n",
    "    centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = list(df.columns)\n",
    "    for i in range(num):\n",
    "        tops[i] = []\n",
    "        for ind in centroids[i, :20]:\n",
    "            tops[i].append(terms[ind])\n",
    "    return tops \n",
    "\n",
    "def elbow(df):\n",
    "    from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "    kelbow_visualizer(KMeans(), df, k=(2, 10),timings=False)\n",
    "    # kelbow_visualizer(KMeans(), df, k=(2, 10),metric='calinski_harabasz',timings=False)\n",
    "    # kelbow_visualizer(KMeans(), df, k=(2, 10),metric='silhouette',timings=False)\n",
    "    \n",
    "def intercluster(model,num):\n",
    "    from yellowbrick.cluster import intercluster_distance\n",
    "    intercluster_distance(KMeans(num),model, embedding='mds') \n",
    "\n",
    "def pcaColors(kmeans,df):\n",
    "    '''\n",
    "    PCA visualization code comes from https://www.askpython.com/python/examples/plot-k-means-clusters-python \\n\",\n",
    "    '''\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_results = pca.fit_transform(df) \n",
    "    label = kmeans.fit_predict(pca_results) \n",
    "    u_labels = np.unique(label)\n",
    "    groupColors = {0:'pink',1:'purple',2:'darkblue',3:'plum',4:'palevioletred'}\n",
    "    for i in u_labels:\n",
    "        plt.scatter(pca_results[label == i , 0] , pca_results[label == i , 1] , label = i,color=groupColors[i])\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.show()\n",
    "\n",
    "def pca(df):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_results = pca.fit_transform(df) \n",
    "    pca_df = pd.DataFrame(pca_results, columns=[\"pc1\",\"pc2\"])\n",
    "    pca_df.plot.scatter(x='pc1', y='pc2')\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re \n",
    "\n",
    "\n",
    "def keywords(csv):\n",
    "    '''\n",
    "    Returns a dictionary in this format {id : (keywords,date)}\n",
    "    '''\n",
    "    df = pd.read_csv(csv)\n",
    "    keywords = df['keywords']\n",
    "    ids = df['id']\n",
    "    dates = df['date']\n",
    "    numFiles = len(ids)\n",
    "    count = 0\n",
    "    dict = {}\n",
    "    while count < numFiles:\n",
    "        words = set(keywords[count].split('--'))\n",
    "        # removing unnecessary keywords\n",
    "        words.discard('')\n",
    "        # Removing unnecessary dates  \n",
    "        newWords = []\n",
    "        for w in words: \n",
    "            w = w.replace('.','')\n",
    "            w = re.sub(r'\\([^)]*\\)','',w)\n",
    "            w = re.sub(r'[.\"\\'-?:!;]', '', w)\n",
    "            w = re.sub(r' ca|-|[0-9]{4}|,','',w)\n",
    "            if re.search('Sultan of the Turks',w):\n",
    "                w = 'Sultan of the Turks'\n",
    "            if re.search('Süleyman',w):\n",
    "                w = 'Süleyman'\n",
    "            w = w.strip()\n",
    "            newWords.append(w)\n",
    "        newWords = set(newWords)\n",
    "        newWords.discard('')\n",
    "        newWords.discard('-')\n",
    "        newWords.discard('th century')\n",
    "        newWords.discard('I')\n",
    "        newWords.discard('Early works to')\n",
    "        newWords.discard('To')\n",
    "        newWords.discard('No Keywords')\n",
    "        newWords.discard('Great Britain')\n",
    "        dict[ids[count]] = (newWords,dates[count])\n",
    "        count += 1\n",
    "    return dict \n",
    "    \n",
    "def getTexts(folder,searchList):\n",
    "    fileToText = {}\n",
    "    underscores = {}\n",
    "    for root,dirs,files in os.walk(folder):\n",
    "        for file in files:\n",
    "                if '.txt' not in file: continue\n",
    "                path = os.path.join(folder,file)\n",
    "                f = open(path,'r')\n",
    "                text = f.readlines()[0]\n",
    "                if '_' in file: \n",
    "                        name = file.split('_')[0]\n",
    "                        if name not in searchList: continue\n",
    "                        if name not in underscores.keys(): \n",
    "                                underscores[name] = text\n",
    "                        else: underscores[name] = underscores[name] + ' ' + text\n",
    "                else: \n",
    "                        name = file.split('.')[0]\n",
    "                        if name not in searchList: continue\n",
    "                        fileToText[name] = text\n",
    "                f.close()\n",
    "        for name,text in underscores.items():\n",
    "            fileToText[name] = text\n",
    "        return fileToText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351\n"
     ]
    }
   ],
   "source": [
    "'''Get Topic Words for each text in a dictionary'''\n",
    "readFile = open('/srv/data/periodFeatures/period4features/period1625topics.txt','r')\n",
    "topics = {}\n",
    "for line in readFile:\n",
    "    tcpID =  line.split(':')[0].strip('.txt')\n",
    "    topic = line.split(':')[1].strip()\n",
    "    topic = re.sub(r' if | of | by | yet | the | man | see | and ',' ',topic)\n",
    "    topic = re.sub('  ',' ',topic)\n",
    "    topics[tcpID] = topic.split(' ')\n",
    "readFile.close()\n",
    "print(len(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351\n",
      "105\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "'''Get additional features'''\n",
    "inFile = open('/srv/data/periodFeatures/period4features/featuresPeriod1625.txt','r')\n",
    "inFileLines = inFile.readlines()\n",
    "inFile.close()\n",
    "featuresDict = {}\n",
    "for line in inFileLines: \n",
    "    line = line.split(':')\n",
    "    features = line[1].strip().split(' ')\n",
    "    featuresDict[line[0].strip()] = features\n",
    "print(len(featuresDict))\n",
    "\n",
    "'''Get drug and tobacco features'''\n",
    "tobacco = 'tobacco|tobaco|tobacca|tobacconist'\n",
    "drug = 'drug|drugge|drugg|elixir|apothecary|confection|confect|medicinable|medicine|medecine|medicament|arsenic|poppy|chemic|medicinal|intoxicate|tacamahaca|potion|mithridate|antimony|opiate|opium'\n",
    "\n",
    "inFile = open('/srv/data/periodFeatures/period4features/drugTobaccoPeriod1625.txt','r')\n",
    "inFileLines = inFile.readlines()\n",
    "inFile.close()\n",
    "tobaccoDrug = {}\n",
    "for line in inFileLines: \n",
    "    line = line.split(':')\n",
    "    features = line[1].strip().split(' ')\n",
    "    if features[0] == '': continue\n",
    "    \n",
    "    '''For specifically tobacco texts'''\n",
    "    instances = re.findall(tobacco,' '.join(features))\n",
    "    if len(instances) < 1: continue\n",
    "    tobaccoDrug[line[0].strip()] = features\n",
    "print(len(tobaccoDrug))\n",
    "\n",
    "'''Get tobacco n-gram features'''\n",
    "inFile = open('/srv/data/periodFeatures/period4features/newtobaccograms.txt','r')\n",
    "inFileLines = inFile.readlines()\n",
    "inFile.close()\n",
    "tobaccoNgrams = {}\n",
    "for line in inFileLines: \n",
    "    line = line.split(':')\n",
    "    features = line[1].strip()\n",
    "    if features == '': continue\n",
    "    tobaccoNgrams[line[0].strip()] = features.strip()\n",
    "print(len(tobaccoNgrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of texts to classify is 81\n"
     ]
    }
   ],
   "source": [
    "'''Get keywords metadata for each text in a dictionary'''\n",
    "'''Create list of feature strings for each file and remove stopwords'''\n",
    "import re \n",
    "from tobaccoTexts import per4\n",
    "features = {}\n",
    "textInfo = getTexts('/srv/data/relevantEPBodySTOP',per4)\n",
    "kwdict = keywords('/srv/data/metadata/tuning/relevant.csv')\n",
    "\n",
    "for ID in tobaccoNgrams.keys():\n",
    "    '''Skip the texts that do not have any mention of tobacco or drugs'''\n",
    "    features[ID] = [textInfo[ID]]\n",
    "    kWords = (list(kwdict[ID][0]))\n",
    "    features[ID].append(' '.join(kWords*50))\n",
    "    features[ID].append(' '.join(topics[ID]*50))\n",
    "    features[ID].append(' '.join(featuresDict[ID]*20))\n",
    "    features[ID].append(' '.join(tobaccoDrug[ID]*20))\n",
    "    features[ID].append(' '.join(tobaccoNgrams[ID]*10))\n",
    "    featureStr = ' '.join(features[ID])\n",
    "    featureStr = re.sub(r' if | of | by | yet | the | man | see | and ',' ',featureStr)\n",
    "    featureStr = re.sub('  ',' ',featureStr)\n",
    "    features[ID] = featureStr\n",
    "print(f'Total number of texts to classify is {len(features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vectorize(features.values(),features.keys(),'tfidf')\n",
    "# pca(df)\n",
    "elbow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "groups,model = cluster(df,features.keys(),k)\n",
    "groups = sorted(groups)\n",
    "tops = topTerms(model,df,k)\n",
    "pcaColors(model,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Write out documentation k-means clustering'''\n",
    "\n",
    "outfile = open('/srv/data/amy/per3doc.txt','a+')\n",
    "for k,IDs in groups:\n",
    "    kWords = []\n",
    "    tWords = []\n",
    "    outfile.write(f'Cluster {k}:')\n",
    "    for TCPID in IDs:\n",
    "        kWords.extend(list(kwdict[TCPID][0]))\n",
    "        tWords.extend(topics[TCPID])\n",
    "    outfile.write(f'<br><ul><li><b>TCP IDs:</b> {IDs}</li>')\n",
    "    outfile.write(f'<li><b>TFIDF terms:</b> {tops[k]}</li>')\n",
    "    outfile.write(f'<li><b>Keywords:</b> {Counter(kWords).most_common(n=10)}</li>')\n",
    "    outfile.write(f'<li><b>Topic words:</b> {Counter(tWords).most_common(n=10)}</li></ul>')\n",
    "outfile.write('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('/srv/data/metadata/tuning/relevant.csv')\n",
    "authorDict = {}\n",
    "for idx,tcpID in enumerate(meta['id']):\n",
    "    if tcpID in features.keys(): \n",
    "        authorDict[tcpID] = []\n",
    "        words = set(meta['author'][idx].split(';'))\n",
    "        words.discard('')\n",
    "        newWords = []\n",
    "        for w in words: \n",
    "            w = w.strip()\n",
    "            if re.search('printer|engraver',w):continue\n",
    "            newWords.append(w)\n",
    "        authorDict[tcpID] = list(set(newWords))\n",
    "for TCPID,authorList in authorDict.items(): \n",
    "    for author in authorList: \n",
    "        if author == 'James, Thomas, 1593?-1635?' : \n",
    "            print(author,': ', TCPID,': ',tobaccoNgrams[TCPID]) \n",
    "        # if author == 'Jonson, Ben, 1573?-1637, attributed name.':\n",
    "        #     print(author,': ', TCPID,': ',tobaccoNgrams[TCPID])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('myPython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84aadb223da975688c2cc3670ed5b0d1d9aed0a6c1d8972bdf32d841a4ac1406"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

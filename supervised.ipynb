{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries Needed\n",
    "import csv\n",
    "import os \n",
    "import re\n",
    "import glob, csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from lxml import etree\n",
    "\n",
    "# Functions for Supervised Classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTexts(folder):\n",
    "    '''\n",
    "    Takes in plain text files and outputs a tuple of lists, with the first being the text\n",
    "    within each file as a string and the second list being the IDs of each text. \n",
    "    '''\n",
    "    textStrings = []\n",
    "    fileNames = []\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder,file)\n",
    "        f = open(path,'r')\n",
    "        text = f.readlines()[0]\n",
    "        textStrings.append(text)\n",
    "        name = file.split('.')[0]\n",
    "        fileNames.append(name)\n",
    "        f.close()\n",
    "    return textStrings,fileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abas</th>\n",
       "      <th>abase</th>\n",
       "      <th>abash</th>\n",
       "      <th>abate</th>\n",
       "      <th>abatement</th>\n",
       "      <th>abbess</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbot</th>\n",
       "      <th>...</th>\n",
       "      <th>zeinel</th>\n",
       "      <th>zeinell</th>\n",
       "      <th>zemes</th>\n",
       "      <th>zermonia</th>\n",
       "      <th>zingis</th>\n",
       "      <th>zizimus</th>\n",
       "      <th>zofala</th>\n",
       "      <th>zolnock</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoroam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A09209</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A03476</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A07886</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A03477</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A12330</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A22250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A68246</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A72397</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00838</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B16236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 11129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aaron   abandon  abas  abase  abash     abate  abatement  abbess  \\\n",
       "A09209    0.0  0.000000   0.0    0.0    0.0  0.000000        0.0     0.0   \n",
       "A03476    0.0  0.000000   0.0    0.0    0.0  0.000000        0.0     0.0   \n",
       "A07886    0.0  0.020718   0.0    0.0    0.0  0.045048        0.0     0.0   \n",
       "A03477    0.0  0.000000   0.0    0.0    0.0  0.000000        0.0     0.0   \n",
       "A12330    0.0  0.000000   0.0    0.0    0.0  0.000000        0.0     0.0   \n",
       "...       ...       ...   ...    ...    ...       ...        ...     ...   \n",
       "A22250    0.0  0.000000   0.0    0.0    0.0  0.000000        0.0     0.0   \n",
       "A68246    0.0  0.000000   0.0    0.0    0.0  0.000000        0.0     0.0   \n",
       "A72397    0.0  0.000000   0.0    0.0    0.0  0.000000        0.0     0.0   \n",
       "B00838    0.0  0.000000   0.0    0.0    0.0  0.000000        0.0     0.0   \n",
       "B16236    0.0  0.000000   0.0    0.0    0.0  0.000000        0.0     0.0   \n",
       "\n",
       "        abbey  abbot  ...  zeinel  zeinell  zemes  zermonia  zingis  zizimus  \\\n",
       "A09209    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "A03476    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "A07886    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "A03477    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "A12330    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "...       ...    ...  ...     ...      ...    ...       ...     ...      ...   \n",
       "A22250    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "A68246    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "A72397    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "B00838    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "B16236    0.0    0.0  ...     0.0      0.0    0.0       0.0     0.0      0.0   \n",
       "\n",
       "        zofala  zolnock  zone  zoroam  \n",
       "A09209     0.0      0.0   0.0     0.0  \n",
       "A03476     0.0      0.0   0.0     0.0  \n",
       "A07886     0.0      0.0   0.0     0.0  \n",
       "A03477     0.0      0.0   0.0     0.0  \n",
       "A12330     0.0      0.0   0.0     0.0  \n",
       "...        ...      ...   ...     ...  \n",
       "A22250     0.0      0.0   0.0     0.0  \n",
       "A68246     0.0      0.0   0.0     0.0  \n",
       "A72397     0.0      0.0   0.0     0.0  \n",
       "B00838     0.0      0.0   0.0     0.0  \n",
       "B16236     0.0      0.0   0.0     0.0  \n",
       "\n",
       "[108 rows x 11129 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fileinfo = getTexts(\"/srv/data/companyTextsClean\")\n",
    "# First we need to create an \"instance\" of the vectorizer, with the proper settings.\n",
    "# Normalization is set to 'l2' by default\n",
    "tfidf = TfidfVectorizer(min_df=2, sublinear_tf=True)\n",
    "# I am choosing to turn on sublinear term frequency scaling, which takes the log of\n",
    "# term frequencies and can help to de-emphasize function words like pronouns and articles. \n",
    "# You might make a different choice depending on your corpus.\n",
    "\n",
    "# Once we've created the instance, we can \"transform\" our counts\n",
    "results = tfidf.fit_transform(fileinfo[0])\n",
    "\n",
    "# Make results readable using Pandas\n",
    "readable_results = pd.DataFrame(results.toarray(), index=fileinfo[1], columns=tfidf.get_feature_names_out()) # Convert information back to a DataFrame\n",
    "readable_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords(csv):\n",
    "    '''\n",
    "    Returns a dictionary in this format {id : (keywords,date)}\n",
    "    '''\n",
    "    df = pd.read_csv(csv)\n",
    "    keywords = df['keywords']\n",
    "    ids = df['id']\n",
    "    dates = df['date']\n",
    "    numFiles = len(ids)\n",
    "    count = 0\n",
    "    dict = {}\n",
    "    while count < numFiles:\n",
    "        words = set(keywords[count].split('--'))\n",
    "        # removing unnecessary keywords\n",
    "        words.discard('')\n",
    "        # Removing unnecessary dates  \n",
    "        newWords = []\n",
    "        for w in words: \n",
    "            w = w.replace('.','')\n",
    "            w = re.sub(r'\\([^)]*\\)','',w)\n",
    "            w = re.sub(r' ca|-|[0-9]{4}|,','',w)\n",
    "            if re.search('Sultan of the Turks',w):\n",
    "                w = 'Sultan of the Turks'\n",
    "            if re.search('Süleyman',w):\n",
    "                w = 'Süleyman'\n",
    "            w = w.strip()\n",
    "            newWords.append(w)\n",
    "        newWords = set(newWords)\n",
    "        newWords.discard('')\n",
    "        newWords.discard('-')\n",
    "        newWords.discard('17th century')\n",
    "        newWords.discard('Early works to')\n",
    "        newWords.discard('To')\n",
    "        newWords.discard('No Keywords')\n",
    "        newWords.discard('Great Britain')\n",
    "        dict[ids[count]] = (newWords,dates[count])\n",
    "        count += 1\n",
    "    return dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A09209'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('/srv/data/metadata/textCounts/eicOurTime.txt','r')\n",
    "text = f.readlines()[0]\n",
    "name = text.split(' -- ')[0]\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_text(txt):\n",
    "    f = open(txt,'r')\n",
    "    dict={}\n",
    "    for text in f.readlines():\n",
    "        name = text.split(' -- ')[0]\n",
    "        keywords = text.split(' -- ')[1]\n",
    "        keywords = keywords.replace('{','')\n",
    "        keywords = keywords.replace('}','')\n",
    "        keywords = keywords.replace(\"'\",'')\n",
    "        keywords = keywords.split(', ')\n",
    "        newWords = []\n",
    "        for w in keywords: \n",
    "            w = w.replace('.','')\n",
    "            w = re.sub(r'\\([^)]*\\)','',w)\n",
    "            w = re.sub(r' ca|-|[0-9]{4}|,','',w)\n",
    "            if re.search('Sultan of the Turks',w):\n",
    "                w = 'Sultan of the Turks'\n",
    "            if re.search('Süleyman',w):\n",
    "                w = 'Süleyman'\n",
    "            w = w.strip()\n",
    "            newWords.append(w)\n",
    "        newWords = set(newWords)\n",
    "        newWords.discard('')\n",
    "        newWords.discard('-')\n",
    "        newWords.discard('17th century')\n",
    "        newWords.discard('Early works to')\n",
    "        newWords.discard('To')\n",
    "        newWords.discard('No Keywords')\n",
    "        newWords.discard('Great Britain')\n",
    "        dict[name] = newWords\n",
    "    return dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "targets=[]\n",
    "\n",
    "dict1 = keywords_text('/srv/data/metadata/textCounts/eicOurTime.txt')\n",
    "dict2 = keywords_text('/srv/data/metadata/textCounts/levantOurTime.txt')\n",
    "dict3 = keywords_text('/srv/data/metadata/textCounts/virginiaOurTime.txt')\n",
    "kwdict = dict1|dict2\n",
    "kwdict = kwdict|dict3\n",
    "#kwdict = keywords('/srv/data/companyCSV/companyCSV.csv')\n",
    "for filekey in kwdict.keys():\n",
    "   if filekey not in ['A08092','B12972','A09573','A15796','B11348','B14268']:\n",
    "    kw = kwdict[filekey]\n",
    "    levant_terms = ['Turkey', 'Sultan of the Turks', 'Mediterranean Sea', 'Süleyman']\n",
    "    eastind_terms = ['East India Company', 'East Indies']\n",
    "    virginia_terms = ['Virginia', 'Virginia Company of London']\n",
    "    if any(k in levant_terms for k in kw):\n",
    "        targets.append('Levant')\n",
    "    elif any(k in eastind_terms for k in kw):\n",
    "        targets.append('East India')\n",
    "    elif any(k in virginia_terms for k in kw):\n",
    "        targets.append('Virginia')\n",
    "    else:\n",
    "        targets.append('Neither')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8979591836734694\n",
      "\n",
      "Results of this run:\n",
      "\n",
      "Play Title | Actual Genre | Predicted Genre\n",
      "A14518 | Virginia | Virginia\n",
      "A04364 | East India | East India\n",
      "A12330 | East India | East India\n",
      "A16711 | Virginia | Virginia\n",
      "A08440 | Virginia | Virginia\n",
      "A14526 | Virginia | Virginia\n",
      "A07559 | Levant | Levant\n",
      "A09478 | Levant | Levant\n",
      "A22537 | Virginia | Virginia\n",
      "A04763 | East India | East India\n",
      "A10725 | Virginia | Virginia\n",
      "A12470 | Virginia | Virginia\n",
      "A09209 | East India | Virginia\n",
      "A13057 | Virginia | Virginia\n",
      "A37552 | East India | East India\n",
      "A19590 | Virginia | Virginia\n",
      "A18686 | Levant | Levant\n",
      "A14514 | Virginia | Virginia\n",
      "B00838 | Virginia | Virginia\n",
      "A15309 | Levant | Levant\n",
      "A03451 | East India | Virginia\n",
      "A17260 | Levant | Levant\n",
      "A04581 | Virginia | Virginia\n",
      "A01836 | Levant | Levant\n",
      "A73966 | East India | Virginia\n",
      "A04911 | Levant | Levant\n",
      "A08162 | Levant | Levant\n",
      "A38817 | Virginia | Virginia\n",
      "A15387 | East India | Levant\n",
      "A02472 | Levant | Levant\n",
      "A14519 | Virginia | Virginia\n",
      "A12466 | Virginia | Virginia\n",
      "A21082 | East India | East India\n",
      "A06425 | Levant | Levant\n",
      "A10314 | Levant | Levant\n",
      "A22250 | Virginia | Virginia\n",
      "A05751 | Levant | Levant\n",
      "A16139 | Levant | Levant\n",
      "A19763 | East India | Virginia\n",
      "A10526 | Levant | Levant\n",
      "B16236 | Virginia | Virginia\n",
      "A13128 | Levant | Levant\n",
      "A08166 | Levant | Levant\n",
      "A21083 | East India | East India\n",
      "A14511 | Virginia | Virginia\n",
      "A01839 | Levant | Levant\n",
      "A02059 | Virginia | Virginia\n",
      "A21084 | East India | East India\n",
      "A22327 | Levant | Levant\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(readable_results, targets, test_size=0.45, random_state=42)\n",
    "lr = LogisticRegression(random_state=0, solver='lbfgs', penalty='none')\n",
    "clf = lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "# evaluate accuracy\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred, normalize=True, sample_weight=None))\n",
    "print()\n",
    "print(\"Results of this run:\\n\")\n",
    "print(\"Play Title | Actual Genre | Predicted Genre\")\n",
    "for title, real, predicted in zip(X_test.index, y_test, y_pred):\n",
    "    print(f\"{title} | {real} | {predicted}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c549b64183ea530b2d0f14882894735bea6c6bc1186b47fc38576af0e6f4ebe4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

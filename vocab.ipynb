{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing the vocabulary of texts at individual word level. see below for word frequency clouds (and numerical counts), TF-IDF scores, and n-grams (bi and tri-grams) :)))\n",
    "\n",
    "code references:\n",
    " - https://earlyprint.org/jupyterbook/tf_idf.html \n",
    " - https://www.machinelearningplus.com/nlp/gensim-tutorial/#10howtocreatebigramsandtrigramsusingphrasermodels \n",
    " - https://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/\n",
    " - https://towardsdatascience.com/generate-meaningful-word-clouds-in-python-5b85f5668eeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from gensim.models import Phrases, phrases\n",
    "\n",
    "def gettexts(folder, searchList):\n",
    "    texts = []\n",
    "    underscores = {}\n",
    "    #list of lists of strings, each text broken up into individual token strings\n",
    "    tokenized = []\n",
    "    #list of texts as a continuous string\n",
    "    textnames = []\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder,file)\n",
    "        f = open(path,'r')\n",
    "        data = f.readlines()[0]\n",
    "        # accounting for underscores in EP filenames\n",
    "        if '_' in file: \n",
    "            name = file.split('_')[0]\n",
    "            if name  in searchList: continue\n",
    "            if name not in underscores.keys(): \n",
    "                    underscores[name] = data\n",
    "            else: underscores[name] = underscores[name] + ' ' + data\n",
    "        else: \n",
    "            name = file.split('.')[0]\n",
    "            if name  in searchList: continue\n",
    "            texts.append(data)\n",
    "            textnames.append(name)\n",
    "        f.close()\n",
    "    for name,text in underscores.items():\n",
    "            texts.append(text)\n",
    "            textnames.append(name)\n",
    "    for text in texts:\n",
    "        #tokenize by white space\n",
    "        words = text.strip().split(' ')\n",
    "        tokenized.append(words)\n",
    "    return [tokenized, texts, textnames]\n",
    "    #i.e. index 0 gives list of tokens, 1 gives list of texts as one string, 2 gives list textnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordclouds generated through term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term frequency & word clouds through wordcloud processing\n",
    "wcdata = gettexts('/srv/data/EPTuningReplaced')\n",
    "wctokens = wcdata[0]\n",
    "wctexts = wcdata[1]\n",
    "wcnames = wcdata[2]\n",
    "fileTF = \"A04813\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for a single text \n",
    "# textstring = ' '.join(wctexts[wcnames.index(fileTF)]).lower()\n",
    "#use this for a collection of texts\n",
    "\n",
    "wholecorpusstring = ' '.join(wctexts)\n",
    "\n",
    "# parameters to play with: min_word_length, collocations, collocation_threshold, stopwards\n",
    "\n",
    "#single text\n",
    "# wordcloud = WordCloud(stopwords=STOPWORDS, collocations=True, min_word_length=3).generate(textstring)\n",
    "#corpus\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS, collocations=True, collocation_threshold=20, min_word_length=4).generate(wholecorpusstring)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#single text\n",
    "# textdict = wordcloud.process_text(textstring)\n",
    "#corpus\n",
    "textdict = wordcloud.process_text(wholecorpusstring)\n",
    "\n",
    "wordfreq={k: v for k, v in sorted(textdict.items(),reverse=True, key=lambda item: item[1])}\n",
    "relfreq=wordcloud.words_\n",
    "\n",
    "# not using this, doesn't print nicely\n",
    "# N=40\n",
    "# print(\"word frequencies:\", list(wordfreq.items())[:N])\n",
    "# print(\"relative frequencies:\", list(relfreq.items())[:N])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputting the numbers for frequencies\n",
    "\n",
    "# combining word frequencies and relative frequencies into one dictionary for cleaner printing\n",
    "result = defaultdict(list)\n",
    "for freq in (wordfreq, relfreq):\n",
    "    for key, value in freq.items():\n",
    "        result[key].append(value)\n",
    "headers = ('absolute frequency', 'relative frequency')\n",
    "print(pd.DataFrame((result.values()), result.keys(), headers).head(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up manual term frequency\n",
    "count = CountVectorizer(ngram_range=(1,3))\n",
    "X = count.fit_transform(wctexts)\n",
    "X = X.toarray()\n",
    "dataframe = pd.DataFrame(X, index =[name for name in wcnames], columns=count.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud generation through term freqs above\n",
    "\n",
    "topstrings = dataframe.loc[fileTF].sort_values(ascending=False)[:4000]\n",
    "textdict = dataframe.loc[fileTF].sort_values(ascending=False).to_dict()\n",
    "\n",
    "wordcloud2 = WordCloud(min_word_length = 3)\n",
    "wordcloud2.generate_from_frequencies(textdict)\n",
    "\n",
    "plt.imshow(wordcloud2, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF analysis: looking at a matrix to compare all texts, extracting TF-IDF scores of a single text, and generating wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfdata = gettexts('/srv/data/EPTuningReplaced')\n",
    "tfidftokens = tfidfdata[0]\n",
    "tfidftexts = tfidfdata[1]\n",
    "tfidfnames = tfidfdata[2]\n",
    "\n",
    "#setting a text to sort by for TF-IDF analysis\n",
    "basetext = 'A01092'\n",
    "\n",
    "#load wordcounts onto dataframe\n",
    "wordcounts = [Counter(t) for t in tfidftokens]\n",
    "df = pd.DataFrame(wordcounts, index=[name for name in tfidfnames]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using transformer, generate table to compare tf-idfs across multiple texts\n",
    "\n",
    "# normalization turned off\n",
    "# sublinear term frequency scaling turned on (takes log of term frequencies and can help to de-emphasize function words like pronouns and articles)\n",
    "tfidf = TfidfTransformer(norm=None, sublinear_tf=True)\n",
    "results = tfidf.fit_transform(df)\n",
    "table = pd.DataFrame(results.toarray(), index=df.index, columns=df.columns)\n",
    "\n",
    "# columns are texts, using .head(25) to show top 25 terms\n",
    "# sort using words with highest tfidf scores in specified basetext as an example\n",
    "table.T.sort_values(by=[basetext], ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer version, but outputting tf-idf values for a single text, easier viewing\n",
    "\n",
    "transformer = TfidfTransformer(norm=None, sublinear_tf=True, use_idf=True)\n",
    "cv = CountVectorizer()\n",
    "wc = cv.fit_transform(tfidftexts)\n",
    "wctrans = transformer.fit_transform(wc)\n",
    "\n",
    "single = pd.DataFrame(wctrans[tfidfnames.index(basetext)].T.todense(), index=cv.get_feature_names_out(), columns=[basetext + \" TF-IDF\"])\n",
    "single = single.sort_values(basetext + ' TF-IDF', ascending=False)\n",
    "\n",
    "print (single.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf wordclouds - cannot just use wordcloud processing (rip)\n",
    "\n",
    "tfidfcloud = WordCloud(min_word_length = 3)\n",
    "tfidfcloud.generate_from_frequencies(single.to_dict()[basetext + ' TF-IDF'])\n",
    "\n",
    "plt.imshow(tfidfcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram generation: denoting training/testing corpus, generating common bigrams (sorted by descending frequency), searching for bigrams containing key terms of interest, and also generating context windows to clarify specific bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change for each time period: index = period - 1\n",
    "period = open('/srv/data/timeranges.txt', 'r').readlines()[1]\n",
    "period = period.strip().strip('[').strip(']').replace(\"'\", '').split(', ')\n",
    "empty = []\n",
    "\n",
    "bigramdata = gettexts('/srv/data/relevantEPBodyNOSTOP', period)\n",
    "bigramtokens = bigramdata[0]\n",
    "bigramtexts = bigramdata[1]\n",
    "bigramnames = bigramdata[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting texts for training/testing by index\n",
    "\n",
    "#given in list of list of strings\n",
    "training = []\n",
    "#given list of strings\n",
    "testing = []\n",
    "testlen = 0\n",
    "\n",
    "#for specific text\n",
    "# testtext = 'A02495'\n",
    "\n",
    "for t in bigramtokens:\n",
    "    #for 50:50 splitting\n",
    "    # if bigramtokens.index(t)%2==1:\n",
    "    if bigramtokens.index(t)%2==0:\n",
    "    \n",
    "    #for running period-specific training/testing\n",
    "    # if bigramnames[bigramtokens.index(t)] not in testnames:\n",
    "\n",
    "    #text-specific\n",
    "    # if bigramnames[bigramtokens.index(t)] != testtext:\n",
    "        training.append(t)\n",
    "    else: \n",
    "        testlen += 1\n",
    "        for word in t:\n",
    "            testing.append(word)\n",
    "\n",
    "print(len(training))\n",
    "print(testlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating bigrams, can take a bit of time lmao\n",
    "\n",
    "# training bigram model: parameters incl min count, threshold (from -1 to 1), scoring (npmi = more robust?), \n",
    "#  and connector words enabled to allow for longer, informative ngrams (e.g. 'trade and traffic')\n",
    "bigrammodel = Phrases(training, min_count = 1, threshold=-0.5, scoring='npmi', connector_words=phrases.ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "# getting the frequency(?) of bigrams within testing set\n",
    "bgcount = Counter(b for b in bigrammodel[testing] if len(b.split(\"_\")) > 1 )\n",
    "\n",
    "# printing top 20 most common bigrams\n",
    "print(pd.DataFrame(dict(bgcount).values(), index=dict(bgcount).keys(), columns=['bigram frequency']).sort_values('bigram frequency', ascending=False).head(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for specific bigrams based on a word of interest\n",
    "searchword = 'tabacco'\n",
    "\n",
    "#for outputting to txt file, specify here\n",
    "# bruh = '/srv/data/joy/'+'/bigrams.txt'\n",
    "# bgoutfile = open(bruh,'a+')\n",
    "\n",
    "searchbigrams = {}\n",
    "for key in dict(bgcount).keys():\n",
    "    # if key.split('_')[0] == searchword or key.split('_')[-1] == searchword:\n",
    "    if searchword in key.split('_'):\n",
    "\n",
    "        #print (key, dict(bgcount)[key])\n",
    "        \n",
    "        #printing out to textfile yee\n",
    "        # bgoutfile.write(key + '\\n')\n",
    "       \n",
    "        searchbigrams[key] = dict(bgcount)[key]\n",
    "\n",
    "# bgoutfile.close()\n",
    "\n",
    "#nice printing, ordered by frequency\n",
    "print(pd.DataFrame(searchbigrams.values(), index=searchbigrams.keys(), columns=['frequency']).sort_values('frequency', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams\n",
    "\n",
    "trigrammodel = Phrases(bigrammodel[training], min_count = 3, threshold = -0.5, scoring='npmi', connector_words=phrases.ENGLISH_CONNECTOR_WORDS )\n",
    "tgcount = Counter(t for t in trigrammodel[testing] if len(t.split(\"_\")) > 2 )\n",
    "print(pd.DataFrame(dict(tgcount).values(), index=dict(tgcount).keys(), columns=['trigram frequency']).sort_values('trigram frequency', ascending=False).head(n=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for terms of interest in trigrams\n",
    "\n",
    "# looking for specific bigrams based on a word of interest\n",
    "searchword = 'tobacco'\n",
    "\n",
    "#for outputting to txt file, specify here\n",
    "scream = '/srv/data/joy/trigrams.txt'\n",
    "tgoutfile = open(scream,'a+')\n",
    "\n",
    "searchtrigrams = {}\n",
    "for key in dict(tgcount).keys():\n",
    "    if key.split('_')[0] == searchword or key.split('_')[-1] == searchword or key.split('_')[1] == searchword:\n",
    "        #print (key, dict(bgcount)[key])\n",
    "        #writing to text file\n",
    "        tgoutfile.write(key + '\\n')\n",
    "        \n",
    "        searchtrigrams[key] = dict(tgcount)[key]\n",
    "\n",
    "#nice printing, ordered by frequency\n",
    "print(pd.DataFrame(searchtrigrams.values(), index=searchtrigrams.keys(), columns=['frequency']).sort_values('frequency', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A64906: the back and have prove these oil good the oil of mardine the oil of alabaster and the oil of water lily hot cause the oil of poppy very good cold cause for scurf the body this infirmity come of choleric and melancholy humour for this\n",
      "A02327: asmuch ounce of turpentine and asmuch rosin oil of olive pound and half pound of oil of bitter almond and asmuch oil of poppy white wax ounce black pitch three ounce melt your gum and heat your oil with the verdegrece and strain\n",
      "A04936: store the second chapter sleep if the sick can not sleep anoint the fore part of the head with oil of water lily and oil of poppy they you may for need add little opium that sleep thereby may provoke note that box without scarification\n",
      "A17310: nightshade opium compound liquid syrupe of poppy violet rose solid nicholai romanun laudanum paracelsi outward use oil of poppy violet rose mandrake nutmeg oderament of vinegar rose water opium frontal of rosecake rosevineger nutmeg\n",
      "A17310: cap mania hildesheim spicel vigil outward use oil of nutmeg extraction expression with rose water anoint the temple oil of poppy nenuphar mandrake purslan violet the same purpose montanus consil much commend odorament of opium vinegar\n",
      "A08911: renew for when they dry they augment the pain but if the pain yield not these must come narcoticke medicine such the oil of poppy of mandrake caraplasme of henbane and sorrel add thereto mallow and marsh mallow of which speak former\n",
      "A08911: houseleek nightshade and the like diacalcitheos describe galen gal lib comp med secund gen dissolve with vinegar oil of poppye and rose of less efficacy nor unguent nor diverse other thing of the same faculty though proper anodyne\n",
      "A08911: of linseed wurt and make the mucilage extract therefrom into cataplasm with some oil of rose and barley meal some put oil of poppye the pulp of citrul gourd beat and incorporate they together and apply this follow medicine have its\n",
      "A08911: humour from the head frontal may also make this manner take of the oil of rose and waterlilly of each two ounce of the oil of poppy half ounce of opium one dram of rose vinegar one ounce of camphire half dram mix they together also nodule\n",
      "A04596: rejoice and from the council and this either take somewhat away give take away when make not feel the evil that the oil of poppy cause sleep and the oil of mandrake make insensible though leg arm cut off have the lord make his servant\n",
      "A05054: wine assuage the pain of the loin and open the straitness make of water seethe with leaf and temper with oil olive oil of poppy anoint the temple therewith for the headache seethe garlic after roast dry and make into powder with\n",
      "A05054: of the seed of lettuce and white poppy and use sleep cause drink spoonful of syrup of poppy anoint the temple with oil of poppy sleep cause take one spoonful of white poppy seed with little possiteale make with violet strawberry leave\n",
      "A14595: will more good strengthen the brain than ounce of the syrup prepare with the same simple likewise three drop of the oil of poppy of the oil of nenuphar will more avail cause sleep and rest than three ounce of their sirrupe moreover\n",
      "A15684: hand and foot wash with decoction of dill camomile lettuce poppy mallow and willow leave and after anoint they with oil of poppy feed expression let smell this opiate follow take of mithridate five grain of opium three of saffron with\n",
      "A09011: procure rest and sleep the leaf the same manner also if the head and temple bathe with the decoction warm with the oil of poppye the green leaf head bruise and apply with little vinegar make into with barley meal and cool and temper\n",
      "A16845: hand and foot wash with the decoction of dill chammomill lettuce poppy mallow and willow leave and anoint they with oil of poppy seed make expression take dram of mithridate five grain of opium three of saffron with spoonful of malmsey\n",
      "A09920: sun and also for the seethe the change of the flower and mix of the decoction this oil have almost the virtue of the oil of poppy but because not cold nor not much dull the sense therefore may mingle with the oil of poppy cause man\n",
      "A09920: almost the virtue of the oil of poppy but because not cold nor not much dull the sense therefore may mingle with the oil of poppy cause man sleep the better alter hot complexion what part soever lay oil of mint take of the leaf of mint\n",
      "['A64906', 'A02327', 'A04936', 'A17310', 'A08911', 'A04596', 'A05054', 'A14595', 'A15684', 'A09011', 'A16845', 'A09920'] 12\n"
     ]
    }
   ],
   "source": [
    "# printing out context windows for a given ngram, double check which dataset used for bigram generation (nostop or stop)\n",
    "# if you need TCP context windows, use select.py to create a new folder, then grep the specific term\n",
    "\n",
    "# add spaces before and after bigram if you are looking for two very specific words, e.g. \"angel men\" and not \"angel mentions\"\n",
    "searchgram = 'oil of poppy'\n",
    "\n",
    "# accounting for flipped instances of bigrams\n",
    "# flipsearch = searchgram.split(' ')[1]+' '+searchgram.split(' ')[0]\n",
    "\n",
    "names = []\n",
    "for text in bigramtexts:\n",
    "    if (searchgram in text): # or (flipsearch in text):\n",
    "        name = bigramnames[bigramtexts.index(text)]\n",
    "        names.append(name)\n",
    "        indices = [i for i in range(len(text)) if text.startswith(searchgram, i)] #or text.startswith(flipsearch, i)]\n",
    "        windows = []\n",
    "        for index in indices:\n",
    "            if index > 120:\n",
    "                window = text[(index-120):(index+120)].split(' ')\n",
    "            if index < 120:\n",
    "                window = text[0:(index+120)].split(' ')\n",
    "            del window[0]\n",
    "            if len(window) > 0:\n",
    "                del window [-1]\n",
    "        # flipindices = [i for i in range(len(text)) if text.startswith(flipsearch, i)]\n",
    "        # for index in flipindices:\n",
    "        #     print(name+':', text[(index-100):(index+100)]) \n",
    "            print(name+':', ' '.join(window ))\n",
    "print(names, len(names))           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e846d22bd0f16198c95c3cd9e93a943714bae29804f16df7e30c04ce21cb422"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing the vocabulary of texts at individual word level. see below for word frequency clouds (and numerical counts), TF-IDF scores, and bigrams :)))\n",
    "\n",
    "code references:\n",
    " - https://earlyprint.org/jupyterbook/tf_idf.html \n",
    " - https://www.machinelearningplus.com/nlp/gensim-tutorial/#10howtocreatebigramsandtrigramsusingphrasermodels \n",
    " - https://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/\n",
    " - https://towardsdatascience.com/generate-meaningful-word-clouds-in-python-5b85f5668eeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required things\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from nltk import ngrams, BigramCollocationFinder\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up\n",
    "\n",
    "#getting texts\n",
    "texts = []\n",
    "textnames = []\n",
    "folder = '/srv/data/sermonsOurTimeBody'\n",
    "for file in os.listdir(folder):\n",
    "    path = os.path.join(folder,file)\n",
    "    f = open(path,'r')\n",
    "    data = f.readlines()[0]\n",
    "    texts.append(data)\n",
    "    name = file.split('.')[0]\n",
    "    textnames.append(name)\n",
    "    f.close()\n",
    " \n",
    "# list of lists of strings, each text broken up into individual token strings\n",
    "tokenized = []\n",
    "for text in texts:\n",
    "    #tokenize by white space\n",
    "    words = text.strip().split(' ')\n",
    "    tokenized.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordclouds generated through term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term frequency & word clouds through wordcloud processing\n",
    "\n",
    "fileTF = \"A04813\"\n",
    "\n",
    "#use this for a single text \n",
    "# textstring = ' '.join(texts[textnames.index(fileTF)]).lower()\n",
    "#use this for a collection of texts\n",
    "wholecorpusstring = ' '.join(texts)\n",
    "\n",
    "# parameters to play with: min_word_length, collocations, collocation_threshold, stopwards\n",
    "\n",
    "#single text\n",
    "# wordcloud = WordCloud(stopwords=STOPWORDS, collocations=True, min_word_length=3).generate(textstring)\n",
    "#corpus\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS, collocations=True, collocation_threshold=20, min_word_length=4).generate(wholecorpusstring)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#single text\n",
    "# textdict = wordcloud.process_text(textstring)\n",
    "#corpus\n",
    "textdict = wordcloud.process_text(wholecorpusstring)\n",
    "\n",
    "wordfreq={k: v for k, v in sorted(textdict.items(),reverse=True, key=lambda item: item[1])}\n",
    "relfreq=wordcloud.words_\n",
    "\n",
    "# not using this, doesn't print nicely\n",
    "# N=40\n",
    "# print(\"word frequencies:\", list(wordfreq.items())[:N])\n",
    "# print(\"relative frequencies:\", list(relfreq.items())[:N])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputting the numbers for frequencies\n",
    "\n",
    "# combining word frequencies and relative frequencies into one dictionary for cleaner printing\n",
    "result = defaultdict(list)\n",
    "for freq in (wordfreq, relfreq):\n",
    "    for key, value in freq.items():\n",
    "        result[key].append(value)\n",
    "headers = ('absolute frequency', 'relative frequency')\n",
    "\n",
    "print(pd.DataFrame((result.values()), result.keys(), headers).head(n=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up manual term frequency\n",
    "\n",
    "count = CountVectorizer(ngram_range=(1,3))\n",
    "X = count.fit_transform(texts)\n",
    "X = X.toarray()\n",
    "dataframe = pd.DataFrame(X, index =[name for name in textnames], columns=count.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud generation through term freqs above\n",
    "\n",
    "topstrings = dataframe.loc[fileTF].sort_values(ascending=False)[:4000]\n",
    "textdict = dataframe.loc[fileTF].sort_values(ascending=False).to_dict()\n",
    "\n",
    "wordcloud2 = WordCloud(min_word_length = 3)\n",
    "wordcloud2.generate_from_frequencies(textdict)\n",
    "\n",
    "plt.imshow(wordcloud2, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF analysis: looking at a matrix to compare all texts, extracting TF-IDF scores of a single text, and generating wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load wordcounts onto dataframe\n",
    "wordcounts = [Counter(t) for t in tokenized]\n",
    "df = pd.DataFrame(wordcounts, index=[name for name in textnames]).fillna(0)\n",
    "\n",
    "#setting a text to sort by for TF-IDF analysis\n",
    "basetext = 'A01092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using transformer, generate table to compare tf-idfs across multiple texts\n",
    "\n",
    "# normalization turned off\n",
    "# sublinear term frequency scaling turned on (takes log of term frequencies and can help to de-emphasize function words like pronouns and articles)\n",
    "tfidf = TfidfTransformer(norm=None, sublinear_tf=True)\n",
    "results = tfidf.fit_transform(df)\n",
    "\n",
    "table = pd.DataFrame(results.toarray(), index=df.index, columns=df.columns)\n",
    "\n",
    "# columns are texts, using .head(25) to show top 25 terms\n",
    "# sort using words with highest tfidf scores in specified basetext as an example\n",
    "table.T.sort_values(by=[basetext], ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer version, but outputting tf-idf values for a single text, easier viewing\n",
    "\n",
    "transformer = TfidfTransformer(norm=None, sublinear_tf=True, use_idf=True)\n",
    "cv = CountVectorizer()\n",
    "wc = cv.fit_transform(texts)\n",
    "wctrans = transformer.fit_transform(wc)\n",
    "\n",
    "single = pd.DataFrame(wctrans[textnames.index(basetext)].T.todense(), index=cv.get_feature_names_out(), columns=[basetext + \" TF-IDF\"])\n",
    "single = single.sort_values(basetext + ' TF-IDF', ascending=False)\n",
    "\n",
    "print (single.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf wordclouds - cannot just use wordcloud processing (rip)\n",
    "\n",
    "tfidfcloud = WordCloud(min_word_length = 3)\n",
    "tfidfcloud.generate_from_frequencies(single.to_dict()[basetext + ' TF-IDF'])\n",
    "\n",
    "plt.imshow(tfidfcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram generation: denoting training/testing corpus and generating common bigrams sorted by descending frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting bigrams through index (every even index goes into training set)\n",
    "\n",
    "training = []\n",
    "testing = []\n",
    "for t in tokenized:\n",
    "    if tokenized.index(t)%2==0:\n",
    "        training.append(t)\n",
    "    else: \n",
    "        for word in t:\n",
    "            testing.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating bigrams\n",
    "\n",
    "# training bigram model: parameters to play with incl min count, threshold, scoring (npmi = more robust?)\n",
    "bigrammodel = Phrases(training, min_count = 3, threshold=-0.5, scoring='npmi')\n",
    "\n",
    "# getting the frequency(?) of bigrams within test\n",
    "bgcount = Counter(b for b in bigrammodel[testing] if len(b.split(\"_\")) > 1 )\n",
    "\n",
    "# printing top 20 most common bigrams\n",
    "print(pd.DataFrame(dict(bgcount).values(), index=dict(bgcount).keys(), columns=['bigram frequency']).sort_values('bigram frequency', ascending=False).head(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for specific bigrams based on a word of interest\n",
    "searchword = 'men'\n",
    "\n",
    "searchbigrams = {}\n",
    "for key in dict(bgcount).keys():\n",
    "    if key.split('_')[0] == searchword or key.split('_')[-1] == searchword:\n",
    "        print (key, dict(bgcount)[key])\n",
    "        #searchbigrams[key] = dict(bgcount)[key]\n",
    "\n",
    "#nice printing, ordered by frequency\n",
    "#print(pd.DataFrame(searchbigrams.values(), index=searchbigrams.keys(), columns=['frequency']).sort_values('frequency', ascending=False).head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e846d22bd0f16198c95c3cd9e93a943714bae29804f16df7e30c04ce21cb422"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing the vocabulary of texts at individual word level. see below for word frequency clouds (and numerical counts), TF-IDF scores, and bigrams :)))\n",
    "\n",
    "code references:\n",
    " - https://earlyprint.org/jupyterbook/tf_idf.html \n",
    " - https://www.machinelearningplus.com/nlp/gensim-tutorial/#10howtocreatebigramsandtrigramsusingphrasermodels \n",
    " - https://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/\n",
    " - https://towardsdatascience.com/generate-meaningful-word-clouds-in-python-5b85f5668eeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required things\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from nltk import ngrams, BigramCollocationFinder\n",
    "from gensim.models import Phrases, phrases\n",
    "\n",
    "def gettexts(folder):\n",
    "    texts = []\n",
    "    #list of lists of strings, each text broken up into individual token strings\n",
    "    tokenized = []\n",
    "    textnames = []\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder,file)\n",
    "        f = open(path,'r')\n",
    "        data = f.readlines()[0]\n",
    "        texts.append(data)\n",
    "        name = file.split('.')[0]\n",
    "        textnames.append(name)\n",
    "        f.close()\n",
    "    for text in texts:\n",
    "        #tokenize by white space\n",
    "        words = text.strip().split(' ')\n",
    "        tokenized.append(words)\n",
    "    return [tokenized, texts, textnames]\n",
    "#i.e. index 0 gives list of tokens, 1 gives list of texts as one string, 2 gives list textnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordclouds generated through term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term frequency & word clouds through wordcloud processing\n",
    "\n",
    "wcdata = gettexts('/srv/data/EPTuningReplaced')\n",
    "\n",
    "wctokens = wcdata[0]\n",
    "wctexts = wcdata[1]\n",
    "wcnames = wcdata[2]\n",
    "fileTF = \"A04813\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for a single text \n",
    "# textstring = ' '.join(wctexts[wcnames.index(fileTF)]).lower()\n",
    "#use this for a collection of texts\n",
    "\n",
    "wholecorpusstring = ' '.join(wctexts)\n",
    "\n",
    "# parameters to play with: min_word_length, collocations, collocation_threshold, stopwards\n",
    "\n",
    "#single text\n",
    "# wordcloud = WordCloud(stopwords=STOPWORDS, collocations=True, min_word_length=3).generate(textstring)\n",
    "#corpus\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS, collocations=True, collocation_threshold=20, min_word_length=4).generate(wholecorpusstring)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#single text\n",
    "# textdict = wordcloud.process_text(textstring)\n",
    "#corpus\n",
    "textdict = wordcloud.process_text(wholecorpusstring)\n",
    "\n",
    "wordfreq={k: v for k, v in sorted(textdict.items(),reverse=True, key=lambda item: item[1])}\n",
    "relfreq=wordcloud.words_\n",
    "\n",
    "# not using this, doesn't print nicely\n",
    "# N=40\n",
    "# print(\"word frequencies:\", list(wordfreq.items())[:N])\n",
    "# print(\"relative frequencies:\", list(relfreq.items())[:N])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputting the numbers for frequencies\n",
    "\n",
    "# combining word frequencies and relative frequencies into one dictionary for cleaner printing\n",
    "result = defaultdict(list)\n",
    "for freq in (wordfreq, relfreq):\n",
    "    for key, value in freq.items():\n",
    "        result[key].append(value)\n",
    "headers = ('absolute frequency', 'relative frequency')\n",
    "\n",
    "print(pd.DataFrame((result.values()), result.keys(), headers).head(n=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up manual term frequency\n",
    "\n",
    "count = CountVectorizer(ngram_range=(1,3))\n",
    "X = count.fit_transform(wctexts)\n",
    "X = X.toarray()\n",
    "dataframe = pd.DataFrame(X, index =[name for name in wcnames], columns=count.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud generation through term freqs above\n",
    "\n",
    "topstrings = dataframe.loc[fileTF].sort_values(ascending=False)[:4000]\n",
    "textdict = dataframe.loc[fileTF].sort_values(ascending=False).to_dict()\n",
    "\n",
    "wordcloud2 = WordCloud(min_word_length = 3)\n",
    "wordcloud2.generate_from_frequencies(textdict)\n",
    "\n",
    "plt.imshow(wordcloud2, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF analysis: looking at a matrix to compare all texts, extracting TF-IDF scores of a single text, and generating wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfdata = gettexts('/srv/data/EPTuningReplaced')\n",
    "\n",
    "tfidftokens = tfidfdata[0]\n",
    "tfidftexts = tfidfdata[1]\n",
    "tfidfnames = tfidfdata[2]\n",
    "\n",
    "#setting a text to sort by for TF-IDF analysis\n",
    "basetext = 'A01092'\n",
    "\n",
    "#load wordcounts onto dataframe\n",
    "wordcounts = [Counter(t) for t in tfidftokens]\n",
    "df = pd.DataFrame(wordcounts, index=[name for name in tfidfnames]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using transformer, generate table to compare tf-idfs across multiple texts\n",
    "\n",
    "# normalization turned off\n",
    "# sublinear term frequency scaling turned on (takes log of term frequencies and can help to de-emphasize function words like pronouns and articles)\n",
    "tfidf = TfidfTransformer(norm=None, sublinear_tf=True)\n",
    "results = tfidf.fit_transform(df)\n",
    "\n",
    "table = pd.DataFrame(results.toarray(), index=df.index, columns=df.columns)\n",
    "\n",
    "# columns are texts, using .head(25) to show top 25 terms\n",
    "# sort using words with highest tfidf scores in specified basetext as an example\n",
    "table.T.sort_values(by=[basetext], ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer version, but outputting tf-idf values for a single text, easier viewing\n",
    "\n",
    "transformer = TfidfTransformer(norm=None, sublinear_tf=True, use_idf=True)\n",
    "cv = CountVectorizer()\n",
    "wc = cv.fit_transform(tfidftexts)\n",
    "wctrans = transformer.fit_transform(wc)\n",
    "\n",
    "single = pd.DataFrame(wctrans[tfidfnames.index(basetext)].T.todense(), index=cv.get_feature_names_out(), columns=[basetext + \" TF-IDF\"])\n",
    "single = single.sort_values(basetext + ' TF-IDF', ascending=False)\n",
    "\n",
    "print (single.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf wordclouds - cannot just use wordcloud processing (rip)\n",
    "\n",
    "tfidfcloud = WordCloud(min_word_length = 3)\n",
    "tfidfcloud.generate_from_frequencies(single.to_dict()[basetext + ' TF-IDF'])\n",
    "\n",
    "plt.imshow(tfidfcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram generation: denoting training/testing corpus and generating common bigrams sorted by descending frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramdata = gettexts('/srv/data/targetCorpusNOSTOP')\n",
    "\n",
    "bigramtokens = bigramdata[0]\n",
    "bigramtexts = bigramdata[1]\n",
    "bigramnames = bigramdata[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting texts by index\n",
    "\n",
    "#given in list of list of strings\n",
    "training = []\n",
    "#given list of strings\n",
    "testing = []\n",
    "#testing = [word for word in gettexts('/srv/data/targetCorpusSTOP')[1]]\n",
    "for t in bigramtokens:\n",
    "    if bigramtokens.index(t)%2==0:\n",
    "        training.append(t)\n",
    "    else: \n",
    "        for word in t:\n",
    "            testing.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            bigram frequency\n",
      "they_have              16956\n",
      "that_they              16174\n",
      "say_that               10826\n",
      "that_have               9903\n",
      "will_not                9714\n",
      "which_have              9100\n",
      "which_they              8920\n",
      "can_not                 8093\n",
      "his_own                 7857\n",
      "that_which              7509\n",
      "have_not                7147\n",
      "they_shall              6992\n",
      "who_have                6817\n",
      "they_will               6587\n",
      "if_they                 6351\n",
      "not_only                6167\n",
      "when_they               6073\n",
      "they_that               5962\n",
      "they_may                5653\n",
      "will_have               5600\n"
     ]
    }
   ],
   "source": [
    "#generating bigrams, can take a bit long lmao\n",
    "\n",
    "# training bigram model: parameters incl min count, threshold (from -1 to 1), scoring (npmi = more robust?)\n",
    "bigrammodel = Phrases(training, min_count = 3, threshold=-0.5, scoring='npmi', connector_words=phrases.ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "# getting the frequency(?) of bigrams within test\n",
    "bgcount = Counter(b for b in bigrammodel[testing] if len(b.split(\"_\")) > 1 )\n",
    "\n",
    "# printing top 20 most common bigrams\n",
    "print(pd.DataFrame(dict(bgcount).values(), index=dict(bgcount).keys(), columns=['bigram frequency']).sort_values('bigram frequency', ascending=False).head(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     frequency\n",
      "his_health                  93\n",
      "perfect_health              66\n",
      "health_but                  54\n",
      "good_health                 53\n",
      "their_health                50\n",
      "soul_health                 42\n",
      "health_his                  32\n",
      "her_health                  27\n",
      "former_health               27\n",
      "health_that                 26\n",
      "health_their                25\n",
      "health_which                22\n",
      "health_the_body             21\n",
      "health_they                 20\n",
      "restore_health              20\n",
      "our_health                  20\n",
      "your_health                 17\n",
      "recover_health              17\n",
      "health_and_strength         17\n",
      "life_and_health             16\n"
     ]
    }
   ],
   "source": [
    "# looking for specific bigrams based on a word of interest\n",
    "searchword = 'health'\n",
    "\n",
    "searchbigrams = {}\n",
    "for key in dict(bgcount).keys():\n",
    "    if key.split('_')[0] == searchword or key.split('_')[-1] == searchword:\n",
    "        #print (key, dict(bgcount)[key])\n",
    "        searchbigrams[key] = dict(bgcount)[key]\n",
    "\n",
    "#nice printing, ordered by frequency\n",
    "print(pd.DataFrame(searchbigrams.values(), index=searchbigrams.keys(), columns=['frequency']).sort_values('frequency', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A02495: e admiral come and remember the very first shoot the discharge shoot little above the belly whereby make unserviceable for good while after without touch any other for that night yet mean honest true \n",
      "A03149: towards the end the reign stephen they lay level with the ground and the few which remain dismantle make unserviceable this care take disable the lord commons home but for keep the seacoast from forei\n",
      "A07834: land extreme weather and winter camp where they have mean refresh they begin die and will have lose make unserviceable if this course have not take hearten they this day and for many day after diverse\n",
      "A10357: ad the footman enforce lift their bow and arrow and dart over their head keep they from moisten and make unserviceable the water but true and understand say homer the mind man ever affect god will the\n",
      "A10357: pon any angle shall force they give ground and fall back upon their next fellow which many entangle make unserviceable lose force they they may easy because the twenty ship which give themselves scope\n",
      "A10357: an otherwise discern which press forward which recoil than rise the dust thus machanidas his engine make unserviceable the interposition his own man such manner the canon hinder from execution most th\n",
      "A14722: army vanquish also the chivalry the enemy fear with some strange fight some firework which they may make unserviceable now if the enemy have any piece ordnance the rear their army if their battalia su\n",
      "A16309: he own liburnica private barge bruise and boulge say svetonius some mischance that the same thereby make unserviceable may stead thereof obtrude that unsound and false bottomed boat her departure and \n",
      "A18711: y liveless and lumpish disposition answer consider the evil disgrace religion dis hartens other and make unserviceable and dispose for tentation distrust and despair consider the benefit lively and ch\n",
      "A35147: antable hardship until have this show wisdom and religion the neglect column and dishonour the body make unserviceable for god whereas god will have this flesh cherish and wise man ever otherwise ephe\n",
      "A43514: tter end the reign king stephen they level the very ground and those few which remain dismantle and make unserviceable the maritime part think sufficient assure those rock and cliff which compass the \n",
      "A57652: inue three day last the help great storm rain which moisten the african target and weapon that they make unserviceable the roman prevail this battle fight near cirtha the ancient town and court masani\n",
      "A57652: more endeavour get shore take and put the oar the turk slay the captain bassai hurt and many galley make unserviceable king charles upon the earnest desire the english ambassador write the sultan beha\n",
      "A57652: king mind beforehand hoist anchor and sea where his fleet storm tear and shatter and the great ship make unserviceable and left britain beside some private quarrel fall out among the borderer about ki\n",
      "A71306: t thirty forty league from england encounter with storm wherein the scourge spend her main mast and make unserviceable for that voyage that force return for england the dread naught the same year perc\n",
      "A71306: e most they the ship leave weak man for when land land about thousand man which the great part dead make unserviceable for the present there above four hundred report dead when his lordship leave the \n",
      "A02239: ugal two other sink upon the coast flanders and the other four force run ground whereby they manner make unserviceable wherein god judgement admire for that those the same galley wherein the netherlan\n",
      "A07466: ace which much annoy his horse brunswick have but three whereof one break the beginning and another make unserviceable the enemy cannon they can not make the fight good notwithstanding that his footma\n",
      "A19622: ll christendom please who from heaven with justice behold all man purpose sudden most strange drown make unserviceable diverse his best ship war under sail come from lisbon and very near the deffine h\n",
      "A01095: ill the haven savona with gravel government who sudden fill the haven thereof with stone and gravel make unserviceable the venetian perceive that the french king open show himself displease with they \n",
      "A01095: rdnance may enter the channel and come the assault but soon finish they perceive many default which make unserviceable for first they can not guide the water next thewaight the burden and ordnance lik\n",
      "A22507: d keep with all diligence for out the issue life if the fountain water muddy miry quick trouble and make unserviceable if the root the tree rot soon turn with wind and weather the heart man if corrupt\n",
      "A07363: outh where find great part the fleet there they repair their ship and the earl take another his own make unserviceable but they keep continual storm and contrary wind until their three month victual m\n",
      "A07363: which lay the down upon his advertisement come the end the say galley defeat some sink and the rest make unserviceable this great service diverse report according the humour passion man the hollander \n",
      "A01108: ay from she for love the iron doubt revenge their prisoner and for harm the last year but thus much make unserviceable agree the elizabeth shall have she fish now they ready depart news bring the capt\n",
      "A07280: d have take and the above mention vessel lubeck other the galleon sink the other two run ground and make unserviceable french man likewise run ground with two ship the one rotterdam and the other ench\n",
      "B14988: commed iasques persia advance our traffic but how many slay our enemy know not only know their ship make unserviceable for the present and report when settle harbour assure that they lose portingalls \n",
      "A12824: he better culverin break and about two the clock the afternoon the other receive flaw and that mean make unserviceable all that can that day mount the whole culverin upon her carriage the same day the\n",
      "A68202: r also the defence they dangerous the life they within that many number diminish many body maim and make unserviceable the residue sore terrify the calamity their companion and friend who they can giv\n",
      "['A02495', 'A03149', 'A07834', 'A10357', 'A14722', 'A16309', 'A18711', 'A35147', 'A43514', 'A57652', 'A71306', 'A02239', 'A07466', 'A19622', 'A01095', 'A22507', 'A07363', 'A01108', 'A07280', 'B14988', 'A12824', 'A68202']\n"
     ]
    }
   ],
   "source": [
    "# printing out context windows for a given bigram, double check which dataset used for bigram generation (nostop or stop)\n",
    "\n",
    "# add spaces before and after bigram if you are looking for two very specific words, e.g. \"angel men\" and not \"angel mentions\"\n",
    "searchbigram = 'unjust sale'\n",
    "\n",
    "# accounting for flipped instances of bigrams\n",
    "flipsearch = searchbigram.spli\n",
    "\n",
    "names = []\n",
    "# for root, dirs, files in os.walk(dir):\n",
    "#         for file in files:\n",
    "#             if file.endswith('.txt'):            \n",
    "#                 name = file.split('.')[0]\n",
    "#                 path = os.path.join(root,file)\n",
    "#                 f = open(path,'r')\n",
    "#                 text = f.readlines()[0]\n",
    "#                 f.close()\n",
    "for text in bigramtexts:\n",
    "    if (searchbigram in text) or (flipsearch in text):\n",
    "        name = bigramnames[bigramtexts.index(text)]\n",
    "        names.append(name)\n",
    "        indices = [i for i in range(len(text)) if text.startswith(searchbigram, i)]\n",
    "        for index in indices:\n",
    "            print(name+':', text[(index-100):(index+100)])        \n",
    "\n",
    "print(names)           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e846d22bd0f16198c95c3cd9e93a943714bae29804f16df7e30c04ce21cb422"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

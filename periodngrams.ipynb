{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of work in vocab.ipynb, this program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim.models import Phrases, phrases\n",
    "\n",
    "def getTexts(folder,searchList):\n",
    "    fileToText = {}\n",
    "    underscores = {}\n",
    "    for root,dirs,files in os.walk(folder):\n",
    "        for file in files:\n",
    "                if '.txt' not in file: continue\n",
    "                path = os.path.join(folder,file)\n",
    "                f = open(path,'r')\n",
    "                text = f.readlines()[0]\n",
    "                if '_' in file: \n",
    "                        name = file.split('_')[0]\n",
    "                        if name not in searchList: continue\n",
    "                        if name not in underscores.keys(): \n",
    "                                underscores[name] = text\n",
    "                        else: underscores[name] = underscores[name] + ' ' + text\n",
    "                else: \n",
    "                        name = file.split('.')[0]\n",
    "                        if name not in searchList: continue\n",
    "                        fileToText[name] = text\n",
    "                f.close()\n",
    "        for name,text in underscores.items():\n",
    "            fileToText[name] = text\n",
    "        return fileToText\n",
    "\n",
    "def collectgrams(keylist, searchword, collectlist):\n",
    "        for key in keylist:\n",
    "                if searchword in key.split('_'):\n",
    "                        collectlist.append(key) \n",
    "\n",
    "def getgrams(txt):\n",
    "    infile = open(txt,'r')\n",
    "    lines = infile.readlines()\n",
    "    infile.close()\n",
    "    allgrams = []\n",
    "    for line in lines: \n",
    "        line = line.split(':')\n",
    "        ngrams = line[1].strip()\n",
    "        if ngrams == '': continue\n",
    "        ngrams = ngrams.strip().split(' ')\n",
    "        for n in ngrams:\n",
    "            allgrams.append(n)\n",
    "    return allgrams \n",
    "\n",
    "#change for each time period: index is period - 1\n",
    "period = open('/srv/data/timeranges.txt', 'r').readlines()[0]\n",
    "period = period.strip().strip('[').strip(']').replace(\"'\", '').split(', ')\n",
    "\n",
    "bigramdata = getTexts('/srv/data/relevantEPBodyNOSTOP', period)\n",
    "bigramtexts = list(bigramdata.values())\n",
    "bigramnames = list(bigramdata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify tobacco/drugs searchword list:\n",
    "searchwords = ['tobacco']\n",
    "# searchwords = ['drug', 'apothecary', 'confection', 'elixir', 'medicinable', 'medicine', 'arsenic', 'medecine', 'medicament', 'poppy', 'chemic', 'medicinal', 'intoxicate', 'potion', 'mithridate', 'antimony', 'opiate', 'opium']\n",
    "\n",
    "remainders = [0, 1, 2]\n",
    "for n in remainders:\n",
    "    training = []\n",
    "    testing = []\n",
    "    for t in bigramtexts:\n",
    "        #separating by thirds, remainders: 0, 1, 2\n",
    "        if bigramtexts.index(t) % 3 == n:\n",
    "            testing.append(t)\n",
    "        else: \n",
    "            words = t.split(' ')\n",
    "            training.append(words)\n",
    "    print('section', n+1, '| training:', len(training), '| testing:', len(testing))\n",
    "\n",
    "    bigrammodel = Phrases(training, min_count=1, threshold=-0.5, scoring='npmi', connector_words=phrases.ENGLISH_CONNECTOR_WORDS)\n",
    "    trigrammodel = Phrases(bigrammodel[training], min_count=1, threshold=-0.5, scoring='npmi', connector_words=phrases.ENGLISH_CONNECTOR_WORDS )\n",
    "    print('models trained')\n",
    "\n",
    "    count = 0\n",
    "    outfile = open('/srv/data/joy/period1tobacco.txt', 'a+')\n",
    "    for text in testing:\n",
    "        #for outputting to txt file, specify here\n",
    "        name = bigramnames[bigramtexts.index(text)]\n",
    "        testtext = text.strip().split(' ')\n",
    "\n",
    "        grams = []\n",
    "        bgcount = Counter(b for b in bigrammodel[testtext] if len(b.split(\"_\")) > 1)\n",
    "        tgcount = Counter(t for t in trigrammodel[testtext] if len(t.split(\"_\")) > 2)\n",
    "        for searchword in searchwords: \n",
    "            collectgrams(dict(bgcount).keys(), searchword, grams) \n",
    "            collectgrams(dict(tgcount).keys(), searchword, grams)\n",
    "        \n",
    "        gramstring = ' '.join(grams)\n",
    "        outfile.write(name+': '+gramstring+\"\\n\")\n",
    "        count += 1\n",
    "        if count%20 == 0:\n",
    "            print(count, \"files processed\")\n",
    "    outfile.close()\n",
    "print('processing complete')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e846d22bd0f16198c95c3cd9e93a943714bae29804f16df7e30c04ce21cb422"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
